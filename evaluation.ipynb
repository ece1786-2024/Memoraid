{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,re,math,copy, random, time,json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from bert_score import score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up llm params \n",
    "models = [\"gpt-4o-2024-05-13\", \"gpt-4o-2024-08-06\", \"gpt-4o-2024-11-20\", \"gpt-4o-mini-2024-07-18\"]\n",
    "openai_params = {\"model\":models[-1],\"temperature\":0, \"max_tokens\":300, \"top_p\":0.5}\n",
    "api_key = \"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\"  # Replace with your key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util functions for generation and evaluation\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file: \n",
    "        data = json.load(file) \n",
    "    return data\n",
    "\n",
    "def format_msg_role(role, prompt):\n",
    "    return {\"role\": role, \"content\": prompt}\n",
    "def write_json(file_path, data):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file: \n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def get_time():\n",
    "    \"\"\"Returns the current date and time in the format 'yymmdd-hhmm'.\"\"\"\n",
    "    return datetime.now().strftime(\"%m%d_%H%M\")\n",
    "\n",
    "\n",
    "def calculate_bert_scores(reference, hypotheses, model_type=\"bert-base-uncased\"):\n",
    "    # BERTScore requires both references and hypotheses to be lists\n",
    "    references = [reference] * len(hypotheses)  # Duplicate reference for each hypothesis\n",
    "\n",
    "    # Calculate BERTScore\n",
    "    P, R, F1 = score(hypotheses, references, model_type=model_type, verbose=False)\n",
    "\n",
    "    # Return precision scores as a list\n",
    "    return P.tolist()\n",
    "\n",
    "def get_bertP(res_list):\n",
    "    for prob_ind, prob_obj in enumerate(res_list):\n",
    "        ref = prob_obj['A']\n",
    "        hyp = res_list[prob_ind]['HA']\n",
    "        bertP = round(calculate_bert_scores(ref, [hyp])[-1],4)\n",
    "        #\n",
    "        res_list[prob_ind]['bertP'] = bertP\n",
    "        res_list[prob_ind]['hj_bert'] = 0.5*bertP + 0.5*float(res_list[prob_ind]['human_judge'])\n",
    "    return res_list\n",
    "\n",
    "def basic_openai_chat_completion(api_key, openai_params, conv_prompt):\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    openai_params.update({\"messages\":conv_prompt})\n",
    "    response = client.chat.completions.create(**openai_params)\n",
    "    answer_msg = response.choices[0].message.content\n",
    "    #\n",
    "    return response, answer_msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_ROOT = \"/home/yuliang/ece1786/proj/Memoraid/Test_Data\"\n",
    "# database information\n",
    "pers_info_json = load_json(os.path.join(TEST_DATA_ROOT, \"Personal_info.json\"))\n",
    "serv_info_json = load_json(os.path.join(TEST_DATA_ROOT, \"Service_info.json\"))\n",
    "\n",
    "# load test dataset\n",
    "qa_easy = load_json(os.path.join(TEST_DATA_ROOT, \"QA_easy.json\"))\n",
    "qa_mid = load_json(os.path.join(TEST_DATA_ROOT, \"QA_mid.json\"))\n",
    "qa_hard = load_json(os.path.join(TEST_DATA_ROOT, \"QA_hard.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system prompt for baseline model\n",
    "sys_prompt = f\"\"\"\n",
    "You are a helpful assistant. You task is to effectively communicate with an Alzheimer's patient (user), answering user's questions with given context.\n",
    "\n",
    "The following are provided context regarding the user:\n",
    "Pseronal information regarding the user:\n",
    "{json.dumps(pers_info_json)}\n",
    "Service information provided by the care provider:\n",
    "{json.dumps(serv_info_json)}\n",
    "\n",
    "Answer the following question:\n",
    "\"\"\"\n",
    "\n",
    "sys_msg = format_msg_role(\"system\", sys_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished: qa_easy -> writing io\n",
      "finished: qa_mid -> writing io\n",
      "finished: qa_hard -> writing io\n"
     ]
    }
   ],
   "source": [
    "# baseline evaluation\n",
    "# result path\n",
    "BASELINE_EVAL_PATH = f\"/home/yuliang/ece1786/proj/eval_results/baseline/{get_time()}\"\n",
    "os.makedirs(BASELINE_EVAL_PATH, exist_ok=True)\n",
    "#\n",
    "test_data_list = {\"qa_easy\":qa_easy, \"qa_mid\":qa_mid, \"qa_hard\":qa_hard}\n",
    "\n",
    "for curr_test_dataset_name, curr_test_dataset in test_data_list.items():\n",
    "    for prob_ind, prob_obj in enumerate(curr_test_dataset):\n",
    "        usr_msg = format_msg_role(\"user\", prob_obj['QD'])\n",
    "        _,agent_res_msg = basic_openai_chat_completion(api_key, openai_params, [sys_msg, usr_msg])\n",
    "        curr_test_dataset[prob_ind]['HA'] = agent_res_msg\n",
    "        #\n",
    "        ref = prob_obj['A']\n",
    "        hyp = agent_res_msg\n",
    "        curr_test_dataset[prob_ind]['bertP'] = round(calculate_bert_scores(ref, [hyp])[-1],4)\n",
    "    #\n",
    "    print(f\"finished: {curr_test_dataset_name} -> writing io | average_bertP: {np.mean([prob_obj['bertP'] for prob_obj in curr_test_dataset]):.4f}\")\n",
    "    write_json(os.path.join(BASELINE_EVAL_PATH, f\"{curr_test_dataset_name}_baseline.json\"), curr_test_dataset)\n",
    "        \n",
    "\n",
    "# write_json(qa_easy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for curr_test_dataset_name, curr_test_dataset in test_data_list.items():\n",
    "    print(f\"finished: {curr_test_dataset_name} -> writing io | average_bertP: {np.mean([prob_obj['bertP'] for prob_obj in curr_test_dataset]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval (individual result analysis only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is for individual testing, where we take each generated answer set and \n",
    "# calcualte the bertP score and human judge score for each answer. Then we calculate the \n",
    "# weighted scores.\n",
    "BASELINE_RATED_EVAL_PATH = \"/home/yuliang/ece1786/proj/res/baseline_rated/\"\n",
    "WKFL_RATED_EVAL_PATH = \"/home/yuliang/ece1786/proj/res/wkfl_rated/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bertP(res_list):\n",
    "    for prob_ind, prob_obj in enumerate(res_list):\n",
    "        ref = prob_obj['A']\n",
    "        hyp = res_list[prob_ind]['HA']\n",
    "        bertP = round(calculate_bert_scores(ref, [hyp])[-1],4)\n",
    "        #\n",
    "        res_list[prob_ind]['bertP'] = bertP\n",
    "        res_list[prob_ind]['hj_bert'] = 0.5*bertP + 0.5*float(res_list[prob_ind]['human_judge'])\n",
    "    return res_list\n",
    "def get_scores(data_list):\n",
    "    total_human_judge = sum(d['human_judge'] for d in data_list)\n",
    "    total_bertP = sum(d['bertP'] for d in data_list)\n",
    "    count = len(data_list)\n",
    "    \n",
    "    return {\n",
    "        \"average_human_judge\": total_human_judge / count,\n",
    "        \"average_bertP\": total_bertP / count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "finished: easy\n",
      "baseline| 0.5873,1.0000,0.7936\n",
      "wkfl| 0.4502,0.9143,0.6823\n",
      "--------------------\n",
      "finished: mid\n",
      "baseline| 0.6184,1.0000,0.8092\n",
      "wkfl| 0.4544,0.9714,0.7129\n",
      "--------------------\n",
      "finished: hard\n",
      "baseline| 0.5405,1.0000,0.7702\n",
      "wkfl| 0.4389,0.9429,0.6909\n"
     ]
    }
   ],
   "source": [
    "# iterate over each level on each individual file for calculating the scores.\n",
    "qa_levels = ['easy', 'mid', 'hard']\n",
    "\n",
    "for level in qa_levels:\n",
    "    wkfl_list =load_json(os.path.join(WKFL_RATED_EVAL_PATH, f\"4OQA_{level}_updated.json\"))\n",
    "    baseline_list = load_json(os.path.join(BASELINE_RATED_EVAL_PATH, f\"qa_{level}_baseline.json\"))\n",
    "    #\n",
    "    wkfl_list_ = get_bertP(wkfl_list)\n",
    "    baseline_list_ = get_bertP(baseline_list)\n",
    "    #\n",
    "    write_json(os.path.join(BASELINE_RATED_EVAL_PATH, f\"bert_qa_{level}_baseline.json\"), wkfl_list_)\n",
    "    write_json(os.path.join(WKFL_RATED_EVAL_PATH, f\"bert_4OQA_{level}_updated.json\"), baseline_list_)\n",
    "    # display the scores \n",
    "    print('-'*20)\n",
    "    print(f\"finished: {level}\")\n",
    "    print(f\"baseline| {np.mean([obj['bertP'] for obj in baseline_list_]):.4f},{np.mean([obj['human_judge'] for obj in baseline_list_]):.4f},{np.mean([obj['hj_bert'] for obj in baseline_list_]):.4f}\")\n",
    "    print(f\"wkfl| {np.mean([obj['bertP'] for obj in wkfl_list_]):.4f},{np.mean([obj['human_judge'] for obj in wkfl_list_]):.4f},{np.mean([obj['hj_bert'] for obj in wkfl_list_]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the tokens from each json file\n",
    "def count_tokens(json_file):\n",
    "    plain_text = json.dumps(json_file)\n",
    "    tokens = plain_text.split()\n",
    "    token_count = len(tokens)\n",
    "    return token_count\n",
    "\n",
    "pers_info_json = load_json(os.path.join(TEST_DATA_ROOT, \"Personal_info.json\"))\n",
    "serv_info_json = load_json(os.path.join(TEST_DATA_ROOT, \"Service_info.json\"))\n",
    "\n",
    "count_tokens(pers_info_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
